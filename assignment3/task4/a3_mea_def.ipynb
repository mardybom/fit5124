{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2192e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe470f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lenet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lenet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, stride=1, padding=0)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(400, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fcb111",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompactCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Realistic architecture an attacker might use knowing the task is digit recognition.\n",
    "    Based on common MNIST tutorial patterns.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CompactCNN, self).__init__()\n",
    "        # Common MNIST CNN pattern: 32->64 channels\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        # Common MNIST FC sizes: 128->10\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFingerprinting:\n",
    "    def __init__(self, model: nn.Module, device: str = 'cpu'):\n",
    "        \"\"\"\n",
    "        Initialize model fingerprinting defense\n",
    "        \n",
    "        Args:\n",
    "            model: The protected model\n",
    "            device: Device to run computations on\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        self.fingerprint_samples = []\n",
    "        self.fingerprint_labels = []\n",
    "        \n",
    "    def generate_boundary_samples(self, \n",
    "                                data_loader: torch.utils.data.DataLoader,\n",
    "                                num_samples: int = 100,\n",
    "                                num_iterations: int = 10,\n",
    "                                step_size: float = 0.01) -> List[Tuple[torch.Tensor, int]]:\n",
    "        \"\"\"\n",
    "        Generate fingerprint samples near decision boundaries using adversarial perturbations\n",
    "        \n",
    "        Args:\n",
    "            data_loader: DataLoader with training/test data\n",
    "            num_samples: Number of fingerprint samples to generate\n",
    "            num_iterations: Number of iterations for boundary search\n",
    "            step_size: Step size for gradient-based boundary search\n",
    "            \n",
    "        Returns:\n",
    "            List of (sample, label) tuples\n",
    "        \"\"\"\n",
    "        print(f\"Generating {num_samples} boundary fingerprint samples...\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        fingerprint_data = []\n",
    "        samples_generated = 0\n",
    "        \n",
    "        with torch.enable_grad():\n",
    "            for batch_idx, (data, target) in enumerate(data_loader):\n",
    "                if samples_generated >= num_samples:\n",
    "                    break\n",
    "                    \n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                \n",
    "                for i in range(min(len(data), num_samples - samples_generated)):\n",
    "                    original_sample = data[i:i+1].clone()\n",
    "                    original_label = target[i].item()\n",
    "                    \n",
    "                    # Generate boundary sample using targeted adversarial attack\n",
    "                    boundary_sample = self._find_boundary_sample(\n",
    "                        original_sample, original_label, num_iterations, step_size\n",
    "                    )\n",
    "                    \n",
    "                    if boundary_sample is not None:\n",
    "                        # Get the label for the boundary sample\n",
    "                        with torch.no_grad():\n",
    "                            output = self.model(boundary_sample)\n",
    "                            boundary_label = output.argmax(dim=1).item()\n",
    "                        \n",
    "                        # Remove batch dimension before storing\n",
    "                        fingerprint_data.append((boundary_sample.squeeze(0).cpu(), boundary_label))\n",
    "                        samples_generated += 1\n",
    "                        \n",
    "                        if samples_generated % 10 == 0:\n",
    "                            print(f\"Generated {samples_generated}/{num_samples} samples\")\n",
    "        \n",
    "        print(f\"Successfully generated {len(fingerprint_data)} fingerprint samples\")\n",
    "        return fingerprint_data\n",
    "    \n",
    "    def _find_boundary_sample(self, \n",
    "                            sample: torch.Tensor, \n",
    "                            original_label: int,\n",
    "                            num_iterations: int, \n",
    "                            step_size: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Find a sample near the decision boundary using gradient-based search\n",
    "        \"\"\"\n",
    "        # Try to find boundary with different target classes\n",
    "        target_classes = [i for i in range(10) if i != original_label]\n",
    "        random.shuffle(target_classes)\n",
    "        \n",
    "        for target_class in target_classes[:3]:  # Try top 3 different classes\n",
    "            perturbed_sample = sample.clone().detach().requires_grad_(True)\n",
    "            \n",
    "            for iteration in range(num_iterations):\n",
    "                output = self.model(perturbed_sample)\n",
    "                \n",
    "                # Loss to move towards target class\n",
    "                target_tensor = torch.tensor([target_class], device=self.device)\n",
    "                loss = F.cross_entropy(output, target_tensor)\n",
    "                \n",
    "                # Compute gradients\n",
    "                self.model.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update sample towards boundary\n",
    "                with torch.no_grad():\n",
    "                    grad_sign = perturbed_sample.grad.sign()\n",
    "                    perturbed_sample -= step_size * grad_sign\n",
    "                    \n",
    "                    # Clamp to valid pixel range (assuming normalized MNIST)\n",
    "                    perturbed_sample.clamp_(-2.5, 2.5)  # Approximate range for normalized MNIST\n",
    "                \n",
    "                perturbed_sample.grad.zero_()\n",
    "                \n",
    "                # Check if we're near boundary (confidence close to 0.5 for binary decision)\n",
    "                with torch.no_grad():\n",
    "                    output = self.model(perturbed_sample)\n",
    "                    probs = F.softmax(output, dim=1)\n",
    "                    max_prob = probs.max().item()\n",
    "                    \n",
    "                    # If we found a sample with uncertain prediction, return it\n",
    "                    if 0.3 < max_prob < 0.7:  # Near decision boundary\n",
    "                        return perturbed_sample.detach()\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def create_fingerprints(self, \n",
    "                          data_loader: torch.utils.data.DataLoader,\n",
    "                          num_samples: int = 100,\n",
    "                          save_path: str = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Create fingerprints for the model (Offline Phase)\n",
    "        \n",
    "        Args:\n",
    "            data_loader: DataLoader with training/test data\n",
    "            num_samples: Number of fingerprint samples to generate\n",
    "            save_path: Path to save fingerprints\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing fingerprint data\n",
    "        \"\"\"\n",
    "        print(\"=== OFFLINE PHASE: Creating Model Fingerprints ===\")\n",
    "        \n",
    "        # Generate boundary samples\n",
    "        fingerprint_data = self.generate_boundary_samples(data_loader, num_samples)\n",
    "        \n",
    "        # Store fingerprints\n",
    "        self.fingerprint_samples = [sample for sample, _ in fingerprint_data]\n",
    "        self.fingerprint_labels = [label for _, label in fingerprint_data]\n",
    "        \n",
    "        fingerprint_dict = {\n",
    "            'samples': self.fingerprint_samples,\n",
    "            'labels': self.fingerprint_labels,\n",
    "            'num_samples': len(self.fingerprint_samples)\n",
    "        }\n",
    "        \n",
    "        # Save fingerprints if path provided\n",
    "        if save_path:\n",
    "            with open(save_path, 'wb') as f:\n",
    "                pickle.dump(fingerprint_dict, f)\n",
    "            print(f\"Fingerprints saved to {save_path}\")\n",
    "        \n",
    "        print(f\"Created {len(self.fingerprint_samples)} fingerprint samples\")\n",
    "        return fingerprint_dict\n",
    "    \n",
    "    def verify_model(self, \n",
    "                    suspect_model: nn.Module, \n",
    "                    threshold: float = 0.8,\n",
    "                    fingerprint_path: str = None) -> Tuple[bool, float, Dict]:\n",
    "        \"\"\"\n",
    "        Verify if a suspect model is a pirated version (Online Phase)\n",
    "        \n",
    "        Args:\n",
    "            suspect_model: The model to verify\n",
    "            threshold: Matching rate threshold for detection\n",
    "            fingerprint_path: Path to load fingerprints from\n",
    "            \n",
    "        Returns:\n",
    "            (is_pirated, matching_rate, detailed_results)\n",
    "        \"\"\"\n",
    "        print(\"=== ONLINE PHASE: Verifying Suspect Model ===\")\n",
    "        \n",
    "        # Load fingerprints if path provided\n",
    "        if fingerprint_path:\n",
    "            with open(fingerprint_path, 'rb') as f:\n",
    "                fingerprint_dict = pickle.load(f)\n",
    "            self.fingerprint_samples = fingerprint_dict['samples']\n",
    "            self.fingerprint_labels = fingerprint_dict['labels']\n",
    "        \n",
    "        if not self.fingerprint_samples:\n",
    "            raise ValueError(\"No fingerprints available. Run create_fingerprints first.\")\n",
    "        \n",
    "        suspect_model.to(self.device)\n",
    "        suspect_model.eval()\n",
    "        \n",
    "        matches = 0\n",
    "        total_samples = len(self.fingerprint_samples)\n",
    "        detailed_results = []\n",
    "        \n",
    "        print(f\"Testing {total_samples} fingerprint samples...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (sample, original_label) in enumerate(zip(self.fingerprint_samples, self.fingerprint_labels)):\n",
    "                sample = sample.to(self.device)\n",
    "                \n",
    "                # Ensure sample has correct dimensions [1, 1, 28, 28] for MNIST\n",
    "                if len(sample.shape) == 3:  # [1, 28, 28]\n",
    "                    sample = sample.unsqueeze(0)  # Add batch dimension -> [1, 1, 28, 28]\n",
    "                elif len(sample.shape) == 2:  # [28, 28]\n",
    "                    sample = sample.unsqueeze(0).unsqueeze(0)  # -> [1, 1, 28, 28]\n",
    "                \n",
    "                # Get prediction from suspect model\n",
    "                suspect_output = suspect_model(sample)\n",
    "                suspect_label = suspect_output.argmax(dim=1).item()\n",
    "                \n",
    "                # Check if labels match\n",
    "                is_match = (suspect_label == original_label)\n",
    "                if is_match:\n",
    "                    matches += 1\n",
    "                \n",
    "                detailed_results.append({\n",
    "                    'sample_idx': i,\n",
    "                    'original_label': original_label,\n",
    "                    'suspect_label': suspect_label,\n",
    "                    'match': is_match\n",
    "                })\n",
    "                \n",
    "                if (i + 1) % 20 == 0:\n",
    "                    print(f\"Processed {i + 1}/{total_samples} samples\")\n",
    "        \n",
    "        matching_rate = matches / total_samples\n",
    "        is_pirated = matching_rate >= threshold\n",
    "        \n",
    "        print(f\"\\n=== VERIFICATION RESULTS ===\")\n",
    "        print(f\"Total fingerprint samples: {total_samples}\")\n",
    "        print(f\"Matching predictions: {matches}\")\n",
    "        print(f\"Matching rate: {matching_rate:.3f}\")\n",
    "        print(f\"Threshold: {threshold:.3f}\")\n",
    "        print(f\"Verdict: {'PIRATED MODEL DETECTED' if is_pirated else 'Model appears legitimate'}\")\n",
    "        \n",
    "        return is_pirated, matching_rate, {\n",
    "            'total_samples': total_samples,\n",
    "            'matches': matches,\n",
    "            'matching_rate': matching_rate,\n",
    "            'threshold': threshold,\n",
    "            'detailed_results': detailed_results\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f65b39a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2cd58a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n",
    "\n",
    "# 50:50 for Lenet:CompCNN train datasets\n",
    "lenet_train_size = int(0.5 * len(train_dataset))\n",
    "compcnn_train_size = len(train_dataset) - lenet_train_size\n",
    "lenet_train_dataset, compcnn_train_dataset = random_split(train_dataset, [lenet_train_size, compcnn_train_size])\n",
    "\n",
    "# test dataset for evaluation\n",
    "test_dataset = datasets.MNIST(\"../data\", train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1fac85db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data loader\n",
    "lenet_train_loader = torch.utils.data.DataLoader(lenet_train_dataset, batch_size=64, shuffle=True)\n",
    "compcnn_train_loader = torch.utils.data.DataLoader(compcnn_train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05b62504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.299041\n",
      "Train Epoch: 1 [640/30000 (2%)]\tLoss: 2.289697\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 2.031714\n",
      "Train Epoch: 1 [1920/30000 (6%)]\tLoss: 1.758418\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 1.493877\n",
      "Train Epoch: 1 [3200/30000 (11%)]\tLoss: 0.871744\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 0.921750\n",
      "Train Epoch: 1 [4480/30000 (15%)]\tLoss: 0.570331\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 0.856902\n",
      "Train Epoch: 1 [5760/30000 (19%)]\tLoss: 0.504076\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 0.613116\n",
      "Train Epoch: 1 [7040/30000 (23%)]\tLoss: 0.298030\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 0.562277\n",
      "Train Epoch: 1 [8320/30000 (28%)]\tLoss: 0.735044\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 0.435347\n",
      "Train Epoch: 1 [9600/30000 (32%)]\tLoss: 0.508466\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 0.326847\n",
      "Train Epoch: 1 [10880/30000 (36%)]\tLoss: 0.541892\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 0.587256\n",
      "Train Epoch: 1 [12160/30000 (41%)]\tLoss: 0.334622\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 0.433797\n",
      "Train Epoch: 1 [13440/30000 (45%)]\tLoss: 0.254495\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 0.139868\n",
      "Train Epoch: 1 [14720/30000 (49%)]\tLoss: 0.164531\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 0.393300\n",
      "Train Epoch: 1 [16000/30000 (53%)]\tLoss: 0.173497\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 0.349268\n",
      "Train Epoch: 1 [17280/30000 (58%)]\tLoss: 0.246709\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 0.235169\n",
      "Train Epoch: 1 [18560/30000 (62%)]\tLoss: 0.134259\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 0.216299\n",
      "Train Epoch: 1 [19840/30000 (66%)]\tLoss: 0.177914\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 0.301545\n",
      "Train Epoch: 1 [21120/30000 (70%)]\tLoss: 0.235512\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 0.359747\n",
      "Train Epoch: 1 [22400/30000 (75%)]\tLoss: 0.150629\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 0.078224\n",
      "Train Epoch: 1 [23680/30000 (79%)]\tLoss: 0.243216\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 0.093305\n",
      "Train Epoch: 1 [24960/30000 (83%)]\tLoss: 0.203218\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 0.236880\n",
      "Train Epoch: 1 [26240/30000 (87%)]\tLoss: 0.255333\n",
      "Train Epoch: 1 [26880/30000 (90%)]\tLoss: 0.179334\n",
      "Train Epoch: 1 [27520/30000 (92%)]\tLoss: 0.382978\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 0.254107\n",
      "Train Epoch: 1 [28800/30000 (96%)]\tLoss: 0.357014\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 0.276672\n",
      "Train Epoch: 2 [0/30000 (0%)]\tLoss: 0.391654\n",
      "Train Epoch: 2 [640/30000 (2%)]\tLoss: 0.028619\n",
      "Train Epoch: 2 [1280/30000 (4%)]\tLoss: 0.165082\n",
      "Train Epoch: 2 [1920/30000 (6%)]\tLoss: 0.039735\n",
      "Train Epoch: 2 [2560/30000 (9%)]\tLoss: 0.208093\n",
      "Train Epoch: 2 [3200/30000 (11%)]\tLoss: 0.082922\n",
      "Train Epoch: 2 [3840/30000 (13%)]\tLoss: 0.249325\n",
      "Train Epoch: 2 [4480/30000 (15%)]\tLoss: 0.198733\n",
      "Train Epoch: 2 [5120/30000 (17%)]\tLoss: 0.229249\n",
      "Train Epoch: 2 [5760/30000 (19%)]\tLoss: 0.217477\n",
      "Train Epoch: 2 [6400/30000 (21%)]\tLoss: 0.210765\n",
      "Train Epoch: 2 [7040/30000 (23%)]\tLoss: 0.188263\n",
      "Train Epoch: 2 [7680/30000 (26%)]\tLoss: 0.271556\n",
      "Train Epoch: 2 [8320/30000 (28%)]\tLoss: 0.216143\n",
      "Train Epoch: 2 [8960/30000 (30%)]\tLoss: 0.218128\n",
      "Train Epoch: 2 [9600/30000 (32%)]\tLoss: 0.050653\n",
      "Train Epoch: 2 [10240/30000 (34%)]\tLoss: 0.196284\n",
      "Train Epoch: 2 [10880/30000 (36%)]\tLoss: 0.229355\n",
      "Train Epoch: 2 [11520/30000 (38%)]\tLoss: 0.177741\n",
      "Train Epoch: 2 [12160/30000 (41%)]\tLoss: 0.391492\n",
      "Train Epoch: 2 [12800/30000 (43%)]\tLoss: 0.299904\n",
      "Train Epoch: 2 [13440/30000 (45%)]\tLoss: 0.138578\n",
      "Train Epoch: 2 [14080/30000 (47%)]\tLoss: 0.195962\n",
      "Train Epoch: 2 [14720/30000 (49%)]\tLoss: 0.079142\n",
      "Train Epoch: 2 [15360/30000 (51%)]\tLoss: 0.074651\n",
      "Train Epoch: 2 [16000/30000 (53%)]\tLoss: 0.281593\n",
      "Train Epoch: 2 [16640/30000 (55%)]\tLoss: 0.240936\n",
      "Train Epoch: 2 [17280/30000 (58%)]\tLoss: 0.243691\n",
      "Train Epoch: 2 [17920/30000 (60%)]\tLoss: 0.205403\n",
      "Train Epoch: 2 [18560/30000 (62%)]\tLoss: 0.240870\n",
      "Train Epoch: 2 [19200/30000 (64%)]\tLoss: 0.136827\n",
      "Train Epoch: 2 [19840/30000 (66%)]\tLoss: 0.165283\n",
      "Train Epoch: 2 [20480/30000 (68%)]\tLoss: 0.164336\n",
      "Train Epoch: 2 [21120/30000 (70%)]\tLoss: 0.097405\n",
      "Train Epoch: 2 [21760/30000 (72%)]\tLoss: 0.140916\n",
      "Train Epoch: 2 [22400/30000 (75%)]\tLoss: 0.250857\n",
      "Train Epoch: 2 [23040/30000 (77%)]\tLoss: 0.159303\n",
      "Train Epoch: 2 [23680/30000 (79%)]\tLoss: 0.287817\n",
      "Train Epoch: 2 [24320/30000 (81%)]\tLoss: 0.308371\n",
      "Train Epoch: 2 [24960/30000 (83%)]\tLoss: 0.139233\n",
      "Train Epoch: 2 [25600/30000 (85%)]\tLoss: 0.158767\n",
      "Train Epoch: 2 [26240/30000 (87%)]\tLoss: 0.260933\n",
      "Train Epoch: 2 [26880/30000 (90%)]\tLoss: 0.098658\n",
      "Train Epoch: 2 [27520/30000 (92%)]\tLoss: 0.124559\n",
      "Train Epoch: 2 [28160/30000 (94%)]\tLoss: 0.298946\n",
      "Train Epoch: 2 [28800/30000 (96%)]\tLoss: 0.141938\n",
      "Train Epoch: 2 [29440/30000 (98%)]\tLoss: 0.044912\n",
      "Train Epoch: 3 [0/30000 (0%)]\tLoss: 0.062776\n",
      "Train Epoch: 3 [640/30000 (2%)]\tLoss: 0.129770\n",
      "Train Epoch: 3 [1280/30000 (4%)]\tLoss: 0.265419\n",
      "Train Epoch: 3 [1920/30000 (6%)]\tLoss: 0.211467\n",
      "Train Epoch: 3 [2560/30000 (9%)]\tLoss: 0.084117\n",
      "Train Epoch: 3 [3200/30000 (11%)]\tLoss: 0.249381\n",
      "Train Epoch: 3 [3840/30000 (13%)]\tLoss: 0.059698\n",
      "Train Epoch: 3 [4480/30000 (15%)]\tLoss: 0.411874\n",
      "Train Epoch: 3 [5120/30000 (17%)]\tLoss: 0.277185\n",
      "Train Epoch: 3 [5760/30000 (19%)]\tLoss: 0.186081\n",
      "Train Epoch: 3 [6400/30000 (21%)]\tLoss: 0.272638\n",
      "Train Epoch: 3 [7040/30000 (23%)]\tLoss: 0.194446\n",
      "Train Epoch: 3 [7680/30000 (26%)]\tLoss: 0.202686\n",
      "Train Epoch: 3 [8320/30000 (28%)]\tLoss: 0.138003\n",
      "Train Epoch: 3 [8960/30000 (30%)]\tLoss: 0.185263\n",
      "Train Epoch: 3 [9600/30000 (32%)]\tLoss: 0.070784\n",
      "Train Epoch: 3 [10240/30000 (34%)]\tLoss: 0.193144\n",
      "Train Epoch: 3 [10880/30000 (36%)]\tLoss: 0.028829\n",
      "Train Epoch: 3 [11520/30000 (38%)]\tLoss: 0.022629\n",
      "Train Epoch: 3 [12160/30000 (41%)]\tLoss: 0.281236\n",
      "Train Epoch: 3 [12800/30000 (43%)]\tLoss: 0.134290\n",
      "Train Epoch: 3 [13440/30000 (45%)]\tLoss: 0.072522\n",
      "Train Epoch: 3 [14080/30000 (47%)]\tLoss: 0.064297\n",
      "Train Epoch: 3 [14720/30000 (49%)]\tLoss: 0.259916\n",
      "Train Epoch: 3 [15360/30000 (51%)]\tLoss: 0.177841\n",
      "Train Epoch: 3 [16000/30000 (53%)]\tLoss: 0.086656\n",
      "Train Epoch: 3 [16640/30000 (55%)]\tLoss: 0.256037\n",
      "Train Epoch: 3 [17280/30000 (58%)]\tLoss: 0.172007\n",
      "Train Epoch: 3 [17920/30000 (60%)]\tLoss: 0.234725\n",
      "Train Epoch: 3 [18560/30000 (62%)]\tLoss: 0.112938\n",
      "Train Epoch: 3 [19200/30000 (64%)]\tLoss: 0.169357\n",
      "Train Epoch: 3 [19840/30000 (66%)]\tLoss: 0.181840\n",
      "Train Epoch: 3 [20480/30000 (68%)]\tLoss: 0.158673\n",
      "Train Epoch: 3 [21120/30000 (70%)]\tLoss: 0.347214\n",
      "Train Epoch: 3 [21760/30000 (72%)]\tLoss: 0.099414\n",
      "Train Epoch: 3 [22400/30000 (75%)]\tLoss: 0.035540\n",
      "Train Epoch: 3 [23040/30000 (77%)]\tLoss: 0.241011\n",
      "Train Epoch: 3 [23680/30000 (79%)]\tLoss: 0.094184\n",
      "Train Epoch: 3 [24320/30000 (81%)]\tLoss: 0.061334\n",
      "Train Epoch: 3 [24960/30000 (83%)]\tLoss: 0.228503\n",
      "Train Epoch: 3 [25600/30000 (85%)]\tLoss: 0.087125\n",
      "Train Epoch: 3 [26240/30000 (87%)]\tLoss: 0.167212\n",
      "Train Epoch: 3 [26880/30000 (90%)]\tLoss: 0.046515\n",
      "Train Epoch: 3 [27520/30000 (92%)]\tLoss: 0.051886\n",
      "Train Epoch: 3 [28160/30000 (94%)]\tLoss: 0.062043\n",
      "Train Epoch: 3 [28800/30000 (96%)]\tLoss: 0.180015\n",
      "Train Epoch: 3 [29440/30000 (98%)]\tLoss: 0.175950\n",
      "Train Epoch: 4 [0/30000 (0%)]\tLoss: 0.036152\n",
      "Train Epoch: 4 [640/30000 (2%)]\tLoss: 0.096862\n",
      "Train Epoch: 4 [1280/30000 (4%)]\tLoss: 0.105686\n",
      "Train Epoch: 4 [1920/30000 (6%)]\tLoss: 0.080287\n",
      "Train Epoch: 4 [2560/30000 (9%)]\tLoss: 0.088997\n",
      "Train Epoch: 4 [3200/30000 (11%)]\tLoss: 0.145501\n",
      "Train Epoch: 4 [3840/30000 (13%)]\tLoss: 0.025947\n",
      "Train Epoch: 4 [4480/30000 (15%)]\tLoss: 0.119767\n",
      "Train Epoch: 4 [5120/30000 (17%)]\tLoss: 0.083441\n",
      "Train Epoch: 4 [5760/30000 (19%)]\tLoss: 0.241001\n",
      "Train Epoch: 4 [6400/30000 (21%)]\tLoss: 0.129057\n",
      "Train Epoch: 4 [7040/30000 (23%)]\tLoss: 0.165185\n",
      "Train Epoch: 4 [7680/30000 (26%)]\tLoss: 0.101922\n",
      "Train Epoch: 4 [8320/30000 (28%)]\tLoss: 0.099332\n",
      "Train Epoch: 4 [8960/30000 (30%)]\tLoss: 0.012668\n",
      "Train Epoch: 4 [9600/30000 (32%)]\tLoss: 0.088077\n",
      "Train Epoch: 4 [10240/30000 (34%)]\tLoss: 0.050895\n",
      "Train Epoch: 4 [10880/30000 (36%)]\tLoss: 0.025842\n",
      "Train Epoch: 4 [11520/30000 (38%)]\tLoss: 0.038885\n",
      "Train Epoch: 4 [12160/30000 (41%)]\tLoss: 0.046659\n",
      "Train Epoch: 4 [12800/30000 (43%)]\tLoss: 0.190948\n",
      "Train Epoch: 4 [13440/30000 (45%)]\tLoss: 0.026807\n",
      "Train Epoch: 4 [14080/30000 (47%)]\tLoss: 0.122525\n",
      "Train Epoch: 4 [14720/30000 (49%)]\tLoss: 0.474846\n",
      "Train Epoch: 4 [15360/30000 (51%)]\tLoss: 0.038980\n",
      "Train Epoch: 4 [16000/30000 (53%)]\tLoss: 0.112025\n",
      "Train Epoch: 4 [16640/30000 (55%)]\tLoss: 0.122755\n",
      "Train Epoch: 4 [17280/30000 (58%)]\tLoss: 0.132348\n",
      "Train Epoch: 4 [17920/30000 (60%)]\tLoss: 0.107406\n",
      "Train Epoch: 4 [18560/30000 (62%)]\tLoss: 0.038808\n",
      "Train Epoch: 4 [19200/30000 (64%)]\tLoss: 0.118100\n",
      "Train Epoch: 4 [19840/30000 (66%)]\tLoss: 0.140219\n",
      "Train Epoch: 4 [20480/30000 (68%)]\tLoss: 0.063691\n",
      "Train Epoch: 4 [21120/30000 (70%)]\tLoss: 0.114064\n",
      "Train Epoch: 4 [21760/30000 (72%)]\tLoss: 0.057523\n",
      "Train Epoch: 4 [22400/30000 (75%)]\tLoss: 0.162137\n",
      "Train Epoch: 4 [23040/30000 (77%)]\tLoss: 0.093920\n",
      "Train Epoch: 4 [23680/30000 (79%)]\tLoss: 0.095056\n",
      "Train Epoch: 4 [24320/30000 (81%)]\tLoss: 0.076460\n",
      "Train Epoch: 4 [24960/30000 (83%)]\tLoss: 0.095446\n",
      "Train Epoch: 4 [25600/30000 (85%)]\tLoss: 0.026261\n",
      "Train Epoch: 4 [26240/30000 (87%)]\tLoss: 0.040634\n",
      "Train Epoch: 4 [26880/30000 (90%)]\tLoss: 0.208886\n",
      "Train Epoch: 4 [27520/30000 (92%)]\tLoss: 0.028681\n",
      "Train Epoch: 4 [28160/30000 (94%)]\tLoss: 0.288881\n",
      "Train Epoch: 4 [28800/30000 (96%)]\tLoss: 0.026871\n",
      "Train Epoch: 4 [29440/30000 (98%)]\tLoss: 0.062144\n",
      "Train Epoch: 5 [0/30000 (0%)]\tLoss: 0.012934\n",
      "Train Epoch: 5 [640/30000 (2%)]\tLoss: 0.014623\n",
      "Train Epoch: 5 [1280/30000 (4%)]\tLoss: 0.107395\n",
      "Train Epoch: 5 [1920/30000 (6%)]\tLoss: 0.161500\n",
      "Train Epoch: 5 [2560/30000 (9%)]\tLoss: 0.024193\n",
      "Train Epoch: 5 [3200/30000 (11%)]\tLoss: 0.166764\n",
      "Train Epoch: 5 [3840/30000 (13%)]\tLoss: 0.051663\n",
      "Train Epoch: 5 [4480/30000 (15%)]\tLoss: 0.126035\n",
      "Train Epoch: 5 [5120/30000 (17%)]\tLoss: 0.078466\n",
      "Train Epoch: 5 [5760/30000 (19%)]\tLoss: 0.401365\n",
      "Train Epoch: 5 [6400/30000 (21%)]\tLoss: 0.071588\n",
      "Train Epoch: 5 [7040/30000 (23%)]\tLoss: 0.258834\n",
      "Train Epoch: 5 [7680/30000 (26%)]\tLoss: 0.084350\n",
      "Train Epoch: 5 [8320/30000 (28%)]\tLoss: 0.212503\n",
      "Train Epoch: 5 [8960/30000 (30%)]\tLoss: 0.088130\n",
      "Train Epoch: 5 [9600/30000 (32%)]\tLoss: 0.064973\n",
      "Train Epoch: 5 [10240/30000 (34%)]\tLoss: 0.150285\n",
      "Train Epoch: 5 [10880/30000 (36%)]\tLoss: 0.159089\n",
      "Train Epoch: 5 [11520/30000 (38%)]\tLoss: 0.329309\n",
      "Train Epoch: 5 [12160/30000 (41%)]\tLoss: 0.171868\n",
      "Train Epoch: 5 [12800/30000 (43%)]\tLoss: 0.249643\n",
      "Train Epoch: 5 [13440/30000 (45%)]\tLoss: 0.149039\n",
      "Train Epoch: 5 [14080/30000 (47%)]\tLoss: 0.191185\n",
      "Train Epoch: 5 [14720/30000 (49%)]\tLoss: 0.357941\n",
      "Train Epoch: 5 [15360/30000 (51%)]\tLoss: 0.094686\n",
      "Train Epoch: 5 [16000/30000 (53%)]\tLoss: 0.113643\n",
      "Train Epoch: 5 [16640/30000 (55%)]\tLoss: 0.073317\n",
      "Train Epoch: 5 [17280/30000 (58%)]\tLoss: 0.149867\n",
      "Train Epoch: 5 [17920/30000 (60%)]\tLoss: 0.016767\n",
      "Train Epoch: 5 [18560/30000 (62%)]\tLoss: 0.137636\n",
      "Train Epoch: 5 [19200/30000 (64%)]\tLoss: 0.154310\n",
      "Train Epoch: 5 [19840/30000 (66%)]\tLoss: 0.147386\n",
      "Train Epoch: 5 [20480/30000 (68%)]\tLoss: 0.090880\n",
      "Train Epoch: 5 [21120/30000 (70%)]\tLoss: 0.084230\n",
      "Train Epoch: 5 [21760/30000 (72%)]\tLoss: 0.226341\n",
      "Train Epoch: 5 [22400/30000 (75%)]\tLoss: 0.009756\n",
      "Train Epoch: 5 [23040/30000 (77%)]\tLoss: 0.050701\n",
      "Train Epoch: 5 [23680/30000 (79%)]\tLoss: 0.015540\n",
      "Train Epoch: 5 [24320/30000 (81%)]\tLoss: 0.050627\n",
      "Train Epoch: 5 [24960/30000 (83%)]\tLoss: 0.112970\n",
      "Train Epoch: 5 [25600/30000 (85%)]\tLoss: 0.063034\n",
      "Train Epoch: 5 [26240/30000 (87%)]\tLoss: 0.302543\n",
      "Train Epoch: 5 [26880/30000 (90%)]\tLoss: 0.178019\n",
      "Train Epoch: 5 [27520/30000 (92%)]\tLoss: 0.194496\n",
      "Train Epoch: 5 [28160/30000 (94%)]\tLoss: 0.144386\n",
      "Train Epoch: 5 [28800/30000 (96%)]\tLoss: 0.199101\n",
      "Train Epoch: 5 [29440/30000 (98%)]\tLoss: 0.049056\n"
     ]
    }
   ],
   "source": [
    "# Build the Lenet model\n",
    "lenet_model = Lenet()\n",
    "\n",
    "# Define the optimizer for model training\n",
    "optimizer = optim.Adadelta(lenet_model.parameters(), lr=1)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "lenet_model.train()\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    for batch_idx, (data, target) in enumerate(lenet_train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = lenet_model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(lenet_train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(lenet_train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "621ddf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0579, Accuracy: 9840/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "lenet_model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = lenet_model(data)\n",
    "        test_loss += F.nll_loss(\n",
    "            output, target, reduction=\"sum\"\n",
    "        ).item()  # sum up batch loss\n",
    "\n",
    "        pred = output.argmax(\n",
    "            dim=1, keepdim=True\n",
    "        )  # get the index of the max log-probability\n",
    "\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print(\n",
    "    \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "        test_loss,\n",
    "        correct,\n",
    "        len(test_loader.dataset),\n",
    "        100.0 * correct / len(test_loader.dataset),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baf65947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(lenet_model.state_dict(), \"mnist_lenet_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c73bcec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.296233\n",
      "Train Epoch: 1 [640/30000 (2%)]\tLoss: 1.768871\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 0.782838\n",
      "Train Epoch: 1 [1920/30000 (6%)]\tLoss: 0.607692\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 0.395882\n",
      "Train Epoch: 1 [3200/30000 (11%)]\tLoss: 0.296795\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 0.172108\n",
      "Train Epoch: 1 [4480/30000 (15%)]\tLoss: 0.299837\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 0.241298\n",
      "Train Epoch: 1 [5760/30000 (19%)]\tLoss: 0.581469\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 0.172319\n",
      "Train Epoch: 1 [7040/30000 (23%)]\tLoss: 0.266466\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 0.447399\n",
      "Train Epoch: 1 [8320/30000 (28%)]\tLoss: 0.166679\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 0.352879\n",
      "Train Epoch: 1 [9600/30000 (32%)]\tLoss: 0.345815\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 0.240938\n",
      "Train Epoch: 1 [10880/30000 (36%)]\tLoss: 0.167653\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 0.102553\n",
      "Train Epoch: 1 [12160/30000 (41%)]\tLoss: 0.417771\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 0.276290\n",
      "Train Epoch: 1 [13440/30000 (45%)]\tLoss: 0.452137\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 0.153079\n",
      "Train Epoch: 1 [14720/30000 (49%)]\tLoss: 0.159030\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 0.175391\n",
      "Train Epoch: 1 [16000/30000 (53%)]\tLoss: 0.225204\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 0.079621\n",
      "Train Epoch: 1 [17280/30000 (58%)]\tLoss: 0.259197\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 0.111435\n",
      "Train Epoch: 1 [18560/30000 (62%)]\tLoss: 0.139528\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 0.087538\n",
      "Train Epoch: 1 [19840/30000 (66%)]\tLoss: 0.192738\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 0.147993\n",
      "Train Epoch: 1 [21120/30000 (70%)]\tLoss: 0.210681\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 0.168851\n",
      "Train Epoch: 1 [22400/30000 (75%)]\tLoss: 0.124850\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 0.072757\n",
      "Train Epoch: 1 [23680/30000 (79%)]\tLoss: 0.148028\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 0.031025\n",
      "Train Epoch: 1 [24960/30000 (83%)]\tLoss: 0.102567\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 0.073078\n",
      "Train Epoch: 1 [26240/30000 (87%)]\tLoss: 0.057121\n",
      "Train Epoch: 1 [26880/30000 (90%)]\tLoss: 0.245362\n",
      "Train Epoch: 1 [27520/30000 (92%)]\tLoss: 0.207078\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 0.181712\n",
      "Train Epoch: 1 [28800/30000 (96%)]\tLoss: 0.029052\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 0.033536\n",
      "Train Epoch: 2 [0/30000 (0%)]\tLoss: 0.263033\n",
      "Train Epoch: 2 [640/30000 (2%)]\tLoss: 0.094883\n",
      "Train Epoch: 2 [1280/30000 (4%)]\tLoss: 0.128860\n",
      "Train Epoch: 2 [1920/30000 (6%)]\tLoss: 0.104726\n",
      "Train Epoch: 2 [2560/30000 (9%)]\tLoss: 0.167444\n",
      "Train Epoch: 2 [3200/30000 (11%)]\tLoss: 0.086724\n",
      "Train Epoch: 2 [3840/30000 (13%)]\tLoss: 0.048436\n",
      "Train Epoch: 2 [4480/30000 (15%)]\tLoss: 0.097709\n",
      "Train Epoch: 2 [5120/30000 (17%)]\tLoss: 0.030707\n",
      "Train Epoch: 2 [5760/30000 (19%)]\tLoss: 0.132547\n",
      "Train Epoch: 2 [6400/30000 (21%)]\tLoss: 0.233343\n",
      "Train Epoch: 2 [7040/30000 (23%)]\tLoss: 0.171393\n",
      "Train Epoch: 2 [7680/30000 (26%)]\tLoss: 0.256033\n",
      "Train Epoch: 2 [8320/30000 (28%)]\tLoss: 0.137736\n",
      "Train Epoch: 2 [8960/30000 (30%)]\tLoss: 0.098688\n",
      "Train Epoch: 2 [9600/30000 (32%)]\tLoss: 0.077074\n",
      "Train Epoch: 2 [10240/30000 (34%)]\tLoss: 0.048383\n",
      "Train Epoch: 2 [10880/30000 (36%)]\tLoss: 0.404432\n",
      "Train Epoch: 2 [11520/30000 (38%)]\tLoss: 0.028161\n",
      "Train Epoch: 2 [12160/30000 (41%)]\tLoss: 0.054804\n",
      "Train Epoch: 2 [12800/30000 (43%)]\tLoss: 0.078523\n",
      "Train Epoch: 2 [13440/30000 (45%)]\tLoss: 0.047660\n",
      "Train Epoch: 2 [14080/30000 (47%)]\tLoss: 0.031313\n",
      "Train Epoch: 2 [14720/30000 (49%)]\tLoss: 0.208702\n",
      "Train Epoch: 2 [15360/30000 (51%)]\tLoss: 0.055463\n",
      "Train Epoch: 2 [16000/30000 (53%)]\tLoss: 0.082410\n",
      "Train Epoch: 2 [16640/30000 (55%)]\tLoss: 0.078764\n",
      "Train Epoch: 2 [17280/30000 (58%)]\tLoss: 0.027382\n",
      "Train Epoch: 2 [17920/30000 (60%)]\tLoss: 0.350389\n",
      "Train Epoch: 2 [18560/30000 (62%)]\tLoss: 0.069637\n",
      "Train Epoch: 2 [19200/30000 (64%)]\tLoss: 0.072706\n",
      "Train Epoch: 2 [19840/30000 (66%)]\tLoss: 0.113342\n",
      "Train Epoch: 2 [20480/30000 (68%)]\tLoss: 0.096891\n",
      "Train Epoch: 2 [21120/30000 (70%)]\tLoss: 0.102441\n",
      "Train Epoch: 2 [21760/30000 (72%)]\tLoss: 0.128026\n",
      "Train Epoch: 2 [22400/30000 (75%)]\tLoss: 0.067763\n",
      "Train Epoch: 2 [23040/30000 (77%)]\tLoss: 0.084669\n",
      "Train Epoch: 2 [23680/30000 (79%)]\tLoss: 0.226701\n",
      "Train Epoch: 2 [24320/30000 (81%)]\tLoss: 0.183387\n",
      "Train Epoch: 2 [24960/30000 (83%)]\tLoss: 0.170944\n",
      "Train Epoch: 2 [25600/30000 (85%)]\tLoss: 0.115195\n",
      "Train Epoch: 2 [26240/30000 (87%)]\tLoss: 0.028929\n",
      "Train Epoch: 2 [26880/30000 (90%)]\tLoss: 0.212805\n",
      "Train Epoch: 2 [27520/30000 (92%)]\tLoss: 0.025405\n",
      "Train Epoch: 2 [28160/30000 (94%)]\tLoss: 0.034892\n",
      "Train Epoch: 2 [28800/30000 (96%)]\tLoss: 0.089802\n",
      "Train Epoch: 2 [29440/30000 (98%)]\tLoss: 0.176703\n",
      "Train Epoch: 3 [0/30000 (0%)]\tLoss: 0.112820\n",
      "Train Epoch: 3 [640/30000 (2%)]\tLoss: 0.125177\n",
      "Train Epoch: 3 [1280/30000 (4%)]\tLoss: 0.070178\n",
      "Train Epoch: 3 [1920/30000 (6%)]\tLoss: 0.186560\n",
      "Train Epoch: 3 [2560/30000 (9%)]\tLoss: 0.042861\n",
      "Train Epoch: 3 [3200/30000 (11%)]\tLoss: 0.029477\n",
      "Train Epoch: 3 [3840/30000 (13%)]\tLoss: 0.156066\n",
      "Train Epoch: 3 [4480/30000 (15%)]\tLoss: 0.081779\n",
      "Train Epoch: 3 [5120/30000 (17%)]\tLoss: 0.070980\n",
      "Train Epoch: 3 [5760/30000 (19%)]\tLoss: 0.018212\n",
      "Train Epoch: 3 [6400/30000 (21%)]\tLoss: 0.031469\n",
      "Train Epoch: 3 [7040/30000 (23%)]\tLoss: 0.028104\n",
      "Train Epoch: 3 [7680/30000 (26%)]\tLoss: 0.023277\n",
      "Train Epoch: 3 [8320/30000 (28%)]\tLoss: 0.104064\n",
      "Train Epoch: 3 [8960/30000 (30%)]\tLoss: 0.084086\n",
      "Train Epoch: 3 [9600/30000 (32%)]\tLoss: 0.181577\n",
      "Train Epoch: 3 [10240/30000 (34%)]\tLoss: 0.009052\n",
      "Train Epoch: 3 [10880/30000 (36%)]\tLoss: 0.008240\n",
      "Train Epoch: 3 [11520/30000 (38%)]\tLoss: 0.004832\n",
      "Train Epoch: 3 [12160/30000 (41%)]\tLoss: 0.015554\n",
      "Train Epoch: 3 [12800/30000 (43%)]\tLoss: 0.011374\n",
      "Train Epoch: 3 [13440/30000 (45%)]\tLoss: 0.062115\n",
      "Train Epoch: 3 [14080/30000 (47%)]\tLoss: 0.130561\n",
      "Train Epoch: 3 [14720/30000 (49%)]\tLoss: 0.037665\n",
      "Train Epoch: 3 [15360/30000 (51%)]\tLoss: 0.023172\n",
      "Train Epoch: 3 [16000/30000 (53%)]\tLoss: 0.029795\n",
      "Train Epoch: 3 [16640/30000 (55%)]\tLoss: 0.007687\n",
      "Train Epoch: 3 [17280/30000 (58%)]\tLoss: 0.144422\n",
      "Train Epoch: 3 [17920/30000 (60%)]\tLoss: 0.103474\n",
      "Train Epoch: 3 [18560/30000 (62%)]\tLoss: 0.103201\n",
      "Train Epoch: 3 [19200/30000 (64%)]\tLoss: 0.159135\n",
      "Train Epoch: 3 [19840/30000 (66%)]\tLoss: 0.093566\n",
      "Train Epoch: 3 [20480/30000 (68%)]\tLoss: 0.098474\n",
      "Train Epoch: 3 [21120/30000 (70%)]\tLoss: 0.113655\n",
      "Train Epoch: 3 [21760/30000 (72%)]\tLoss: 0.130855\n",
      "Train Epoch: 3 [22400/30000 (75%)]\tLoss: 0.029295\n",
      "Train Epoch: 3 [23040/30000 (77%)]\tLoss: 0.115719\n",
      "Train Epoch: 3 [23680/30000 (79%)]\tLoss: 0.047995\n",
      "Train Epoch: 3 [24320/30000 (81%)]\tLoss: 0.165771\n",
      "Train Epoch: 3 [24960/30000 (83%)]\tLoss: 0.025181\n",
      "Train Epoch: 3 [25600/30000 (85%)]\tLoss: 0.082084\n",
      "Train Epoch: 3 [26240/30000 (87%)]\tLoss: 0.127325\n",
      "Train Epoch: 3 [26880/30000 (90%)]\tLoss: 0.128071\n",
      "Train Epoch: 3 [27520/30000 (92%)]\tLoss: 0.182184\n",
      "Train Epoch: 3 [28160/30000 (94%)]\tLoss: 0.013288\n",
      "Train Epoch: 3 [28800/30000 (96%)]\tLoss: 0.040084\n",
      "Train Epoch: 3 [29440/30000 (98%)]\tLoss: 0.029266\n",
      "Train Epoch: 4 [0/30000 (0%)]\tLoss: 0.039087\n",
      "Train Epoch: 4 [640/30000 (2%)]\tLoss: 0.042795\n",
      "Train Epoch: 4 [1280/30000 (4%)]\tLoss: 0.097075\n",
      "Train Epoch: 4 [1920/30000 (6%)]\tLoss: 0.005593\n",
      "Train Epoch: 4 [2560/30000 (9%)]\tLoss: 0.028962\n",
      "Train Epoch: 4 [3200/30000 (11%)]\tLoss: 0.061675\n",
      "Train Epoch: 4 [3840/30000 (13%)]\tLoss: 0.018349\n",
      "Train Epoch: 4 [4480/30000 (15%)]\tLoss: 0.005170\n",
      "Train Epoch: 4 [5120/30000 (17%)]\tLoss: 0.094605\n",
      "Train Epoch: 4 [5760/30000 (19%)]\tLoss: 0.023708\n",
      "Train Epoch: 4 [6400/30000 (21%)]\tLoss: 0.080498\n",
      "Train Epoch: 4 [7040/30000 (23%)]\tLoss: 0.038678\n",
      "Train Epoch: 4 [7680/30000 (26%)]\tLoss: 0.021805\n",
      "Train Epoch: 4 [8320/30000 (28%)]\tLoss: 0.058623\n",
      "Train Epoch: 4 [8960/30000 (30%)]\tLoss: 0.052050\n",
      "Train Epoch: 4 [9600/30000 (32%)]\tLoss: 0.047556\n",
      "Train Epoch: 4 [10240/30000 (34%)]\tLoss: 0.078012\n",
      "Train Epoch: 4 [10880/30000 (36%)]\tLoss: 0.016574\n",
      "Train Epoch: 4 [11520/30000 (38%)]\tLoss: 0.014945\n",
      "Train Epoch: 4 [12160/30000 (41%)]\tLoss: 0.005085\n",
      "Train Epoch: 4 [12800/30000 (43%)]\tLoss: 0.077692\n",
      "Train Epoch: 4 [13440/30000 (45%)]\tLoss: 0.040923\n",
      "Train Epoch: 4 [14080/30000 (47%)]\tLoss: 0.022850\n",
      "Train Epoch: 4 [14720/30000 (49%)]\tLoss: 0.045013\n",
      "Train Epoch: 4 [15360/30000 (51%)]\tLoss: 0.040175\n",
      "Train Epoch: 4 [16000/30000 (53%)]\tLoss: 0.026401\n",
      "Train Epoch: 4 [16640/30000 (55%)]\tLoss: 0.014908\n",
      "Train Epoch: 4 [17280/30000 (58%)]\tLoss: 0.040918\n",
      "Train Epoch: 4 [17920/30000 (60%)]\tLoss: 0.024714\n",
      "Train Epoch: 4 [18560/30000 (62%)]\tLoss: 0.032597\n",
      "Train Epoch: 4 [19200/30000 (64%)]\tLoss: 0.020215\n",
      "Train Epoch: 4 [19840/30000 (66%)]\tLoss: 0.031586\n",
      "Train Epoch: 4 [20480/30000 (68%)]\tLoss: 0.058915\n",
      "Train Epoch: 4 [21120/30000 (70%)]\tLoss: 0.047072\n",
      "Train Epoch: 4 [21760/30000 (72%)]\tLoss: 0.067253\n",
      "Train Epoch: 4 [22400/30000 (75%)]\tLoss: 0.021527\n",
      "Train Epoch: 4 [23040/30000 (77%)]\tLoss: 0.042940\n",
      "Train Epoch: 4 [23680/30000 (79%)]\tLoss: 0.082480\n",
      "Train Epoch: 4 [24320/30000 (81%)]\tLoss: 0.016624\n",
      "Train Epoch: 4 [24960/30000 (83%)]\tLoss: 0.043767\n",
      "Train Epoch: 4 [25600/30000 (85%)]\tLoss: 0.011020\n",
      "Train Epoch: 4 [26240/30000 (87%)]\tLoss: 0.048894\n",
      "Train Epoch: 4 [26880/30000 (90%)]\tLoss: 0.020892\n",
      "Train Epoch: 4 [27520/30000 (92%)]\tLoss: 0.060489\n",
      "Train Epoch: 4 [28160/30000 (94%)]\tLoss: 0.156202\n",
      "Train Epoch: 4 [28800/30000 (96%)]\tLoss: 0.063825\n",
      "Train Epoch: 4 [29440/30000 (98%)]\tLoss: 0.051629\n",
      "Train Epoch: 5 [0/30000 (0%)]\tLoss: 0.114338\n",
      "Train Epoch: 5 [640/30000 (2%)]\tLoss: 0.015821\n",
      "Train Epoch: 5 [1280/30000 (4%)]\tLoss: 0.022147\n",
      "Train Epoch: 5 [1920/30000 (6%)]\tLoss: 0.007979\n",
      "Train Epoch: 5 [2560/30000 (9%)]\tLoss: 0.095348\n",
      "Train Epoch: 5 [3200/30000 (11%)]\tLoss: 0.013507\n",
      "Train Epoch: 5 [3840/30000 (13%)]\tLoss: 0.142763\n",
      "Train Epoch: 5 [4480/30000 (15%)]\tLoss: 0.007483\n",
      "Train Epoch: 5 [5120/30000 (17%)]\tLoss: 0.027254\n",
      "Train Epoch: 5 [5760/30000 (19%)]\tLoss: 0.007073\n",
      "Train Epoch: 5 [6400/30000 (21%)]\tLoss: 0.023002\n",
      "Train Epoch: 5 [7040/30000 (23%)]\tLoss: 0.044013\n",
      "Train Epoch: 5 [7680/30000 (26%)]\tLoss: 0.077567\n",
      "Train Epoch: 5 [8320/30000 (28%)]\tLoss: 0.052322\n",
      "Train Epoch: 5 [8960/30000 (30%)]\tLoss: 0.032043\n",
      "Train Epoch: 5 [9600/30000 (32%)]\tLoss: 0.032350\n",
      "Train Epoch: 5 [10240/30000 (34%)]\tLoss: 0.033128\n",
      "Train Epoch: 5 [10880/30000 (36%)]\tLoss: 0.060675\n",
      "Train Epoch: 5 [11520/30000 (38%)]\tLoss: 0.018391\n",
      "Train Epoch: 5 [12160/30000 (41%)]\tLoss: 0.003560\n",
      "Train Epoch: 5 [12800/30000 (43%)]\tLoss: 0.012921\n",
      "Train Epoch: 5 [13440/30000 (45%)]\tLoss: 0.005741\n",
      "Train Epoch: 5 [14080/30000 (47%)]\tLoss: 0.002371\n",
      "Train Epoch: 5 [14720/30000 (49%)]\tLoss: 0.017835\n",
      "Train Epoch: 5 [15360/30000 (51%)]\tLoss: 0.039306\n",
      "Train Epoch: 5 [16000/30000 (53%)]\tLoss: 0.029678\n",
      "Train Epoch: 5 [16640/30000 (55%)]\tLoss: 0.022704\n",
      "Train Epoch: 5 [17280/30000 (58%)]\tLoss: 0.002223\n",
      "Train Epoch: 5 [17920/30000 (60%)]\tLoss: 0.064665\n",
      "Train Epoch: 5 [18560/30000 (62%)]\tLoss: 0.045859\n",
      "Train Epoch: 5 [19200/30000 (64%)]\tLoss: 0.020490\n",
      "Train Epoch: 5 [19840/30000 (66%)]\tLoss: 0.080251\n",
      "Train Epoch: 5 [20480/30000 (68%)]\tLoss: 0.148482\n",
      "Train Epoch: 5 [21120/30000 (70%)]\tLoss: 0.093425\n",
      "Train Epoch: 5 [21760/30000 (72%)]\tLoss: 0.005480\n",
      "Train Epoch: 5 [22400/30000 (75%)]\tLoss: 0.006667\n",
      "Train Epoch: 5 [23040/30000 (77%)]\tLoss: 0.018935\n",
      "Train Epoch: 5 [23680/30000 (79%)]\tLoss: 0.007993\n",
      "Train Epoch: 5 [24320/30000 (81%)]\tLoss: 0.112026\n",
      "Train Epoch: 5 [24960/30000 (83%)]\tLoss: 0.115063\n",
      "Train Epoch: 5 [25600/30000 (85%)]\tLoss: 0.027054\n",
      "Train Epoch: 5 [26240/30000 (87%)]\tLoss: 0.010441\n",
      "Train Epoch: 5 [26880/30000 (90%)]\tLoss: 0.052679\n",
      "Train Epoch: 5 [27520/30000 (92%)]\tLoss: 0.015210\n",
      "Train Epoch: 5 [28160/30000 (94%)]\tLoss: 0.003661\n",
      "Train Epoch: 5 [28800/30000 (96%)]\tLoss: 0.033453\n",
      "Train Epoch: 5 [29440/30000 (98%)]\tLoss: 0.111258\n"
     ]
    }
   ],
   "source": [
    "# Build the CompactCNN model\n",
    "comp_cnn_model = CompactCNN()\n",
    "\n",
    "# Define the optimizer for model training\n",
    "optimizer = optim.Adadelta(comp_cnn_model.parameters(), lr=1)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "comp_cnn_model.train()\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    for batch_idx, (data, target) in enumerate(compcnn_train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = comp_cnn_model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(compcnn_train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(compcnn_train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d019742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0289, Accuracy: 9901/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "comp_cnn_model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = comp_cnn_model(data)\n",
    "        test_loss += F.nll_loss(\n",
    "            output, target, reduction=\"sum\"\n",
    "        ).item()  # sum up batch loss\n",
    "\n",
    "        pred = output.argmax(\n",
    "            dim=1, keepdim=True\n",
    "        )  # get the index of the max log-probability\n",
    "\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print(\n",
    "    \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "        test_loss,\n",
    "        correct,\n",
    "        len(test_loader.dataset),\n",
    "        100.0 * correct / len(test_loader.dataset),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57bdbba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(comp_cnn_model.state_dict(), \"mnist_comp_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09ea7b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MEA parameters\n",
    "attack_number = 3000  # maximum attack_number is 30 000\n",
    "attack_indices = random.sample(range(0, len(lenet_train_dataset)), attack_number)\n",
    "\n",
    "queries = torch.utils.data.Subset(lenet_train_dataset, attack_indices)\n",
    "queries_loader = torch.utils.data.DataLoader(queries, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e611de56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 47/47 [00:12<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 1.1324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 47/47 [00:09<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.3811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 47/47 [00:04<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.2384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 47/47 [00:04<00:00,  9.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.1623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 47/47 [00:04<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.1416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 47/47 [00:04<00:00,  9.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 47/47 [00:04<00:00,  9.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.0904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 47/47 [00:04<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.0818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 47/47 [00:04<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.0768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 47/47 [00:04<00:00, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.0715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 47/47 [00:04<00:00, 10.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.0756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 47/47 [00:04<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.0629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 47/47 [00:04<00:00, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.0592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 47/47 [00:04<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.0495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 47/47 [00:10<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.0464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 47/47 [00:04<00:00,  9.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.0460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 47/47 [00:05<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.0472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 47/47 [00:05<00:00,  9.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.0436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 47/47 [00:05<00:00,  8.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.0434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 47/47 [00:05<00:00,  8.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.0441\n",
      "\n",
      "==================================================\n",
      "Model extraction completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate extracted model using realistic attacker architecture\n",
    "extracted_model = CompactCNN()\n",
    "\n",
    "# Define the optimizer for model extracting (optimized for knowledge distillation)\n",
    "optimizer = optim.Adam(extracted_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=25, gamma=0.5)\n",
    "\n",
    "# Perform model extraction\n",
    "lenet_model.eval()\n",
    "\n",
    "for epoch in range(20):\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for data, target in tqdm(queries_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        output = extracted_model(data)\n",
    "\n",
    "        # Query target model without computing gradients\n",
    "        with torch.no_grad():\n",
    "            target_output = lenet_model(data)\n",
    "\n",
    "        loss = F.kl_div(F.log_softmax(output, dim=1), F.softmax(target_output, dim=1), reduction='batchmean', log_target=False)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running statistics\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    print(f\"Avg Loss: {epoch_loss/num_batches:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Model extraction completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce23f1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:09<00:00, 17.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0778, Accuracy: 9765/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "extracted_model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in tqdm(test_loader):\n",
    "        output = extracted_model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction=\"sum\").item()  # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print(\n",
    "    \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "        test_loss,\n",
    "        correct,\n",
    "        len(test_loader.dataset),\n",
    "        100.0 * correct / len(test_loader.dataset),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8fb6c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(extracted_model.state_dict(), \"mnist_comp_cnn_mea_10.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d3e3540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained model successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the original model\n",
    "original_model = Lenet()\n",
    "\n",
    "original_model.load_state_dict(torch.load('mnist_lenet_cnn.pt', map_location=device))\n",
    "print(\"Loaded trained model successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9864f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize fingerprinting system\n",
    "fingerprinting = ModelFingerprinting(original_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f5bc426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OFFLINE PHASE: Creating Model Fingerprints ===\n",
      "Generating 100 boundary fingerprint samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W614 07:01:09.896140905 NNPACK.cpp:62] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10/100 samples\n",
      "Generated 20/100 samples\n",
      "Generated 30/100 samples\n",
      "Generated 40/100 samples\n",
      "Generated 50/100 samples\n",
      "Generated 60/100 samples\n",
      "Generated 70/100 samples\n",
      "Generated 80/100 samples\n",
      "Generated 90/100 samples\n",
      "Generated 100/100 samples\n",
      "Successfully generated 100 fingerprint samples\n",
      "Fingerprints saved to mnist_lenet_cnn_fingerprints.pkl\n",
      "Created 100 fingerprint samples\n"
     ]
    }
   ],
   "source": [
    "# OFFLINE PHASE: Create fingerprints\n",
    "fingerprints = fingerprinting.create_fingerprints(\n",
    "    test_loader, \n",
    "    num_samples=100,  # Generate 100 fingerprint samples\n",
    "    save_path='mnist_lenet_cnn_fingerprints.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4314b227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing with original model (should show high matching rate):\n",
      "=== ONLINE PHASE: Verifying Suspect Model ===\n",
      "Testing 100 fingerprint samples...\n",
      "Processed 20/100 samples\n",
      "Processed 40/100 samples\n",
      "Processed 60/100 samples\n",
      "Processed 80/100 samples\n",
      "Processed 100/100 samples\n",
      "\n",
      "=== VERIFICATION RESULTS ===\n",
      "Total fingerprint samples: 100\n",
      "Matching predictions: 100\n",
      "Matching rate: 1.000\n",
      "Threshold: 0.800\n",
      "Verdict: PIRATED MODEL DETECTED\n"
     ]
    }
   ],
   "source": [
    "# ONLINE PHASE: Test verification with the same model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing with original model (should show high matching rate):\")\n",
    "is_pirated, matching_rate, results = fingerprinting.verify_model(\n",
    "    original_model, \n",
    "    threshold=0.8,\n",
    "    fingerprint_path='mnist_lenet_cnn_fingerprints.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f9819e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained model successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the suspect model\n",
    "comp_cnn_model = CompactCNN()\n",
    "\n",
    "comp_cnn_model.load_state_dict(torch.load('mnist_comp_cnn.pt', map_location=device))\n",
    "print(\"Loaded trained model successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de85a79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing with Independent model:\n",
      "=== ONLINE PHASE: Verifying Suspect Model ===\n",
      "Testing 100 fingerprint samples...\n",
      "Processed 20/100 samples\n",
      "Processed 40/100 samples\n",
      "Processed 60/100 samples\n",
      "Processed 80/100 samples\n",
      "Processed 100/100 samples\n",
      "\n",
      "=== VERIFICATION RESULTS ===\n",
      "Total fingerprint samples: 100\n",
      "Matching predictions: 83\n",
      "Matching rate: 0.830\n",
      "Threshold: 0.800\n",
      "Verdict: PIRATED MODEL DETECTED\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing with Independent model:\")\n",
    "is_pirated_diff, matching_rate_diff, results_diff = fingerprinting.verify_model(\n",
    "    comp_cnn_model,\n",
    "    threshold=0.8,\n",
    "    fingerprint_path='mnist_lenet_cnn_fingerprints.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a780d4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained model successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the suspect model\n",
    "mea_model = CompactCNN()\n",
    "\n",
    "mea_model.load_state_dict(torch.load('mnist_comp_cnn_mea_10.pt', map_location=device))\n",
    "print(\"Loaded trained model successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "441bb154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing with MEA 10% model:\n",
      "=== ONLINE PHASE: Verifying Suspect Model ===\n",
      "Testing 100 fingerprint samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W615 07:19:43.771706053 NNPACK.cpp:62] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20/100 samples\n",
      "Processed 40/100 samples\n",
      "Processed 60/100 samples\n",
      "Processed 80/100 samples\n",
      "Processed 100/100 samples\n",
      "\n",
      "=== VERIFICATION RESULTS ===\n",
      "Total fingerprint samples: 100\n",
      "Matching predictions: 76\n",
      "Matching rate: 0.760\n",
      "Threshold: 0.800\n",
      "Verdict: Model appears legitimate\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing with MEA 10% model:\")\n",
    "is_pirated_diff, matching_rate_diff, results_diff = fingerprinting.verify_model(\n",
    "    mea_model,\n",
    "    threshold=0.8,\n",
    "    fingerprint_path='mnist_lenet_cnn_fingerprints.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41601cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained model successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the suspect model\n",
    "mea_model = CompactCNN()\n",
    "\n",
    "mea_model.load_state_dict(torch.load('mnist_comp_cnn_mea_30.pt', map_location=device))\n",
    "print(\"Loaded trained model successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eff73f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing with MEA 30% model:\n",
      "=== ONLINE PHASE: Verifying Suspect Model ===\n",
      "Testing 100 fingerprint samples...\n",
      "Processed 20/100 samples\n",
      "Processed 40/100 samples\n",
      "Processed 60/100 samples\n",
      "Processed 80/100 samples\n",
      "Processed 100/100 samples\n",
      "\n",
      "=== VERIFICATION RESULTS ===\n",
      "Total fingerprint samples: 100\n",
      "Matching predictions: 84\n",
      "Matching rate: 0.840\n",
      "Threshold: 0.800\n",
      "Verdict: PIRATED MODEL DETECTED\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing with MEA 30% model:\")\n",
    "is_pirated_diff, matching_rate_diff, results_diff = fingerprinting.verify_model(\n",
    "    mea_model,\n",
    "    threshold=0.8,\n",
    "    fingerprint_path='mnist_lenet_cnn_fingerprints.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef6c3f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained model successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the suspect model\n",
    "mea_model = CompactCNN()\n",
    "\n",
    "mea_model.load_state_dict(torch.load('mnist_comp_cnn_mea_50.pt', map_location=device))\n",
    "print(\"Loaded trained model successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f993bb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing with MEA 50% model:\n",
      "=== ONLINE PHASE: Verifying Suspect Model ===\n",
      "Testing 100 fingerprint samples...\n",
      "Processed 20/100 samples\n",
      "Processed 40/100 samples\n",
      "Processed 60/100 samples\n",
      "Processed 80/100 samples\n",
      "Processed 100/100 samples\n",
      "\n",
      "=== VERIFICATION RESULTS ===\n",
      "Total fingerprint samples: 100\n",
      "Matching predictions: 84\n",
      "Matching rate: 0.840\n",
      "Threshold: 0.800\n",
      "Verdict: PIRATED MODEL DETECTED\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing with MEA 50% model:\")\n",
    "is_pirated_diff, matching_rate_diff, results_diff = fingerprinting.verify_model(\n",
    "    mea_model,\n",
    "    threshold=0.8,\n",
    "    fingerprint_path='mnist_lenet_cnn_fingerprints.pkl'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
